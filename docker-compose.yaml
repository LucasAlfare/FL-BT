services:
  api:
    build: .
    ports:
      - "8000:8000"
    depends_on:
      - redis
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 60s
      timeout: 5s
      retries: 3
    volumes:
      - shared-temp:/app/temp

  worker:
    build: .
    command:
      [
        "/app/.venv/bin/celery",
        "-A", "server.celery_worker",
        "worker",
        "--loglevel=info",
        "--concurrency=1",
        "--max-tasks-per-child=1"
      ]
    environment:
      - PYTHONPATH=/app
    depends_on:
      - redis
    # tensorflow/keras is probably making some kind of
    # memory leak between process, or the memory is not
    # being properly deallocated. So the most handful
    # solution is to increase mem_limit for a high value
    # and avoid request too many songs to the Celery queue.
    mem_limit: 14g
    cpus: 2.0
    oom_kill_disable: false
    volumes:
      - shared-temp:/app/temp

  redis:
    image: redis:8.0.0-alpine
    ports:
      - "6379:6379"
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 3s
      timeout: 3s
      retries: 5

# shared volume to API access file generated by the worker
volumes:
  shared-temp: